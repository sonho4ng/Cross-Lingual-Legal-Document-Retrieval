{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11675358,"sourceType":"datasetVersion","datasetId":7301793},{"sourceId":11679732,"sourceType":"datasetVersion","datasetId":7330529},{"sourceId":11680128,"sourceType":"datasetVersion","datasetId":7330751},{"sourceId":237839045,"sourceType":"kernelVersion"},{"sourceId":237847883,"sourceType":"kernelVersion"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# /kaggle/input/crawl-data/top_40_vi_en_v1.csv\n# /kaggle/input/cross-lingual-cross-encoder/cross_encoder_en_vi_epoch3\nimport json\nfrom datasets import Dataset\nimport pandas as pd\nimport numpy as np\ntrain_path=\"/kaggle/input/crawl-data/train_crosslingual.jsonl\"\ntest_path=\"/kaggle/input/crawl-data/test_crosslingual.jsonl\"\neval_path=\"/kaggle/input/crawl-data/eval_crosslingual.jsonl\"\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:44:42.783912Z","iopub.execute_input":"2025-05-05T05:44:42.784658Z","iopub.status.idle":"2025-05-05T05:44:42.788433Z","shell.execute_reply.started":"2025-05-05T05:44:42.784620Z","shell.execute_reply":"2025-05-05T05:44:42.787679Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"import json\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.nn.functional import cosine_similarity\n\ndef l2_normalize(x):\n    return x / x.norm(p=2, dim=-1, keepdim=True)\n\ndef get_embedding(texts, model, tokenizer, device, max_length=256):\n    inputs = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = model(**inputs)\n        if hasattr(outputs, \"pooler_output\") and outputs.pooler_output is not None:\n            embeddings = outputs.pooler_output\n        else:\n            embeddings = outputs.last_hidden_state[:, 0]\n    return l2_normalize(embeddings)\n\ndef embedding_en_corpus(train_df, model, tokenizer, device, batch_size=64, max_length=256):\n    all_contexts = set()\n    for _, row in train_df.iterrows():\n        pos_items = eval(row['en_pos']) if isinstance(row['en_pos'], str) else row['en_pos']\n        neg_items = eval(row['en_neg']) if isinstance(row['en_neg'], str) else row['en_neg']\n        all_contexts.update(pos_items)\n        all_contexts.update(neg_items)\n    all_contexts = list(all_contexts)\n\n    print(\"Encoding EN corpus...\")\n    corpus_embeddings = []\n    for i in range(0, len(all_contexts), batch_size):\n        batch = all_contexts[i:i+batch_size]\n        emb = get_embedding(batch, model, tokenizer, device, max_length)\n        corpus_embeddings.append(emb.cpu())  # keep CPU for later cosine similarity\n    corpus_embeddings = torch.cat(corpus_embeddings, dim=0)  # shape: (N, D)\n    return all_contexts, corpus_embeddings\n\ndef embedding_vi_corpus(train_df, model, tokenizer, device, batch_size=64, max_length=256):\n    all_contexts = set()\n    for _, row in train_df.iterrows():\n        pos_items = eval(row['vi_pos']) if isinstance(row['vi_pos'], str) else row['vi_pos']\n        neg_items = eval(row['vi_neg']) if isinstance(row['vi_neg'], str) else row['vi_neg']\n        all_contexts.update(pos_items)\n        all_contexts.update(neg_items)\n    all_contexts = list(all_contexts)\n\n    print(\"Encoding VI corpus...\")\n    corpus_embeddings = []\n    for i in range(0, len(all_contexts), batch_size):\n        batch = all_contexts[i:i+batch_size]\n        emb = get_embedding(batch, model, tokenizer, device, max_length)\n        corpus_embeddings.append(emb.cpu())  # keep CPU for cosine sim\n    corpus_embeddings = torch.cat(corpus_embeddings, dim=0)\n    return all_contexts, corpus_embeddings\n\ndef vi_query_and_en_context(\n    train_df,\n    all_contexts,\n    corpus_embeddings,\n    model,\n    tokenizer,\n    device,\n    top_k=10,\n    max_length=256\n):\n    results = []\n    corpus_embeddings = corpus_embeddings.to(device)\n\n    for _, row in tqdm(train_df.iterrows(), total=len(train_df)):\n        query = row['vi_query']\n        pos_items = eval(row['en_pos']) if isinstance(row['en_pos'], str) else row['en_pos']\n        pos_items = set(str(p).strip() for p in pos_items)\n\n        q_emb = get_embedding([query], model, tokenizer, device, max_length)  # shape: (1, D)\n        sims = cosine_similarity(q_emb, corpus_embeddings, dim=1)  # shape: (N,)\n        top_indices = sims.topk(top_k).indices.cpu().tolist()\n        top_contexts = [str(all_contexts[i]).strip() for i in top_indices]\n\n        results.append({\n            \"query\": query,\n            \"pos\": list(pos_items),\n            \"top_k_pred\": top_contexts\n        })\n\n    return pd.DataFrame(results)\ndef en_query_and_vi_context(\n    train_df,\n    all_contexts,\n    corpus_embeddings,\n    model,\n    tokenizer,\n    device,\n    top_k=10,\n    max_length=256\n):\n    results = []\n    corpus_embeddings = corpus_embeddings.to(device)\n\n    for _, row in tqdm(train_df.iterrows(), total=len(train_df)):\n        query = row['en_query']\n        pos_items = eval(row['vi_pos']) if isinstance(row['vi_pos'], str) else row['vi_pos']\n        pos_items = set(str(p).strip() for p in pos_items)\n\n        q_emb = get_embedding([query], model, tokenizer, device, max_length)  # shape: (1, D)\n        sims = cosine_similarity(q_emb, corpus_embeddings, dim=1)  # shape: (N,)\n        top_indices = sims.topk(top_k).indices.cpu().tolist()\n        top_contexts = [str(all_contexts[i]).strip() for i in top_indices]\n\n        results.append({\n            \"query\": query,\n            \"pos\": list(pos_items),\n            \"top_k_pred\": top_contexts\n        })\n\n    return pd.DataFrame(results)\ndef vi_query_and_vi_context(\n    train_df,\n    all_contexts,\n    corpus_embeddings,\n    model,\n    tokenizer,\n    device,\n    top_k=10,\n    max_length=256\n):\n    results = []\n    corpus_embeddings = corpus_embeddings.to(device)\n\n    for _, row in tqdm(train_df.iterrows(), total=len(train_df)):\n        query = row['vi_query']\n        pos_items = eval(row['vi_pos']) if isinstance(row['vi_pos'], str) else row['vi_pos']\n        pos_items = set(str(p).strip() for p in pos_items)\n\n        q_emb = get_embedding([query], model, tokenizer, device, max_length)  # shape: (1, D)\n        sims = cosine_similarity(q_emb, corpus_embeddings, dim=1)  # shape: (N,)\n        top_indices = sims.topk(top_k).indices.cpu().tolist()\n        top_contexts = [str(all_contexts[i]).strip() for i in top_indices]\n\n        results.append({\n            \"query\": query,\n            \"pos\": list(pos_items),\n            \"top_k_pred\": top_contexts\n        })\n\n    return pd.DataFrame(results)\ndef en_query_and_en_context(\n    train_df,\n    all_contexts,\n    corpus_embeddings,\n    model,\n    tokenizer,\n    device,\n    top_k=10,\n    max_length=256\n):\n    results = []\n    corpus_embeddings = corpus_embeddings.to(device)\n\n    for _, row in tqdm(train_df.iterrows(), total=len(train_df)):\n        query = row['en_query']\n        pos_items = eval(row['en_pos']) if isinstance(row['en_pos'], str) else row['en_pos']\n        pos_items = set(str(p).strip() for p in pos_items)\n\n        q_emb = get_embedding([query], model, tokenizer, device, max_length)  # shape: (1, D)\n        sims = cosine_similarity(q_emb, corpus_embeddings, dim=1)  # shape: (N,)\n        top_indices = sims.topk(top_k).indices.cpu().tolist()\n        top_contexts = [str(all_contexts[i]).strip() for i in top_indices]\n\n        results.append({\n            \"query\": query,\n            \"pos\": list(pos_items),\n            \"top_k_pred\": top_contexts\n        })\n\n    return pd.DataFrame(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:01:39.089583Z","iopub.execute_input":"2025-05-05T06:01:39.090117Z","iopub.status.idle":"2025-05-05T06:01:39.111818Z","shell.execute_reply.started":"2025-05-05T06:01:39.090093Z","shell.execute_reply":"2025-05-05T06:01:39.110973Z"}},"outputs":[],"execution_count":91},{"cell_type":"code","source":"def compute_recall_mrr_multi_gt(df, k=10):\n    acc1, acc5, acc10 = [], [], []\n    recall10, mrr10 = [], []\n\n    for _, row in df.iterrows():\n        true_texts = set(str(x).strip() for x in row['pos'])\n        pred_texts = [str(x).strip() for x in row['top_k_pred'][:k]]\n\n        # ACC@k: ít nhất 1 đúng trong top-k\n        acc1.append(1 if any(pred in true_texts for pred in pred_texts[:1]) else 0)\n        acc5.append(1 if any(pred in true_texts for pred in pred_texts[:5]) else 0)\n        acc10.append(1 if any(pred in true_texts for pred in pred_texts[:10]) else 0)\n\n        # Recall@10: tỷ lệ ground-truth nằm trong top-10\n        recall = len(set(pred_texts) & true_texts) / len(true_texts)\n        recall10.append(recall)\n\n        # MRR@10: reciprocal rank của đúng đầu tiên trong top-10\n        mrr = 0\n        for rank, pred in enumerate(pred_texts, start=1):\n            if pred in true_texts:\n                mrr = 1 / rank\n                break\n        mrr10.append(mrr)\n\n    # Kết quả trung bình toàn bộ truy vấn\n    print(f\"Accuracy@1:  {np.mean(acc1):.4f}\")\n    print(f\"Accuracy@5:  {np.mean(acc5):.4f}\")\n    print(f\"Accuracy@10: {np.mean(acc10):.4f}\")\n    print(f\"Recall@10:   {np.mean(recall10):.4f}\")\n    print(f\"MRR@10:      {np.mean(mrr10):.4f}\")\n\n    return {\n        \"acc@1\": np.mean(acc1),\n        \"acc@5\": np.mean(acc5),\n        \"acc@10\": np.mean(acc10),\n        \"recall@10\": np.mean(recall10),\n        \"mrr@10\": np.mean(mrr10),\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:21:31.125116Z","iopub.execute_input":"2025-05-05T05:21:31.125379Z","iopub.status.idle":"2025-05-05T05:21:31.132812Z","shell.execute_reply.started":"2025-05-05T05:21:31.125360Z","shell.execute_reply":"2025-05-05T05:21:31.132161Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"def compute_recall_mrr_multi_gt(df, k=10):\n    acc1, acc5, acc_k = [], [], []\n    recall_k, mrr_k = [], []\n\n    for _, row in df.iterrows():\n        true_texts = set(str(x).strip() for x in row['pos'])\n        pred_texts = [str(x).strip() for x in row['top_k_pred'][:k]]\n\n        # ACC@1\n        acc1.append(1 if any(pred in true_texts for pred in pred_texts[:1]) else 0)\n\n        # ACC@5\n        acc5.append(1 if any(pred in true_texts for pred in pred_texts[:5]) else 0)\n\n        # ACC@k\n        acc_k.append(1 if any(pred in true_texts for pred in pred_texts) else 0)\n\n        # Recall@k\n        recall = len(set(pred_texts) & true_texts) / len(true_texts)\n        recall_k.append(recall)\n\n        # MRR@k\n        mrr = 0\n        for rank, pred in enumerate(pred_texts, start=1):\n            if pred in true_texts:\n                mrr = 1 / rank\n                break\n        mrr_k.append(mrr)\n\n    # In kết quả\n    print(f\"Accuracy@1:  {np.mean(acc1):.4f}\")\n    print(f\"Accuracy@5:  {np.mean(acc5):.4f}\")\n    print(f\"Accuracy@{k}: {np.mean(acc_k):.4f}\")\n    print(f\"Recall@{k}:   {np.mean(recall_k):.4f}\")\n    print(f\"MRR@{k}:      {np.mean(mrr_k):.4f}\")\n\n    return {\n        \"acc@1\": np.mean(acc1),\n        \"acc@5\": np.mean(acc5),\n        f\"acc@{k}\": np.mean(acc_k),\n        f\"recall@{k}\": np.mean(recall_k),\n        f\"mrr@{k}\": np.mean(mrr_k),\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:21:31.524654Z","iopub.execute_input":"2025-05-05T05:21:31.524892Z","iopub.status.idle":"2025-05-05T05:21:31.533093Z","shell.execute_reply.started":"2025-05-05T05:21:31.524875Z","shell.execute_reply":"2025-05-05T05:21:31.532345Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_name = \"BAAI/bge-m3\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name).to(device)\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T04:43:01.779473Z","iopub.execute_input":"2025-05-05T04:43:01.780044Z","iopub.status.idle":"2025-05-05T04:43:05.674907Z","shell.execute_reply.started":"2025-05-05T04:43:01.780024Z","shell.execute_reply":"2025-05-05T04:43:05.674292Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"XLMRobertaModel(\n  (embeddings): XLMRobertaEmbeddings(\n    (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n    (position_embeddings): Embedding(8194, 1024, padding_idx=1)\n    (token_type_embeddings): Embedding(1, 1024)\n    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): XLMRobertaEncoder(\n    (layer): ModuleList(\n      (0-23): 24 x XLMRobertaLayer(\n        (attention): XLMRobertaAttention(\n          (self): XLMRobertaSdpaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): XLMRobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): XLMRobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): XLMRobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): XLMRobertaPooler(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (activation): Tanh()\n  )\n)"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"model_name = \"BAAI/bge-m3\"\ncheckpoint_path = \"/kaggle/input/checkpoint2/checkpoint/loss1\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name).to(device)\ntrain_df = pd.read_json(train_path, lines=True)\nvi_contexts, vi_corpus_embeddings = embedding_vi_corpus(train_df, model, tokenizer, device)\nen_contexts, en_corpus_embeddings = embedding_en_corpus(train_df, model, tokenizer, device)\n\n# Predict top-10\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:45:00.518910Z","iopub.execute_input":"2025-05-05T05:45:00.519644Z","iopub.status.idle":"2025-05-05T05:50:43.909056Z","shell.execute_reply.started":"2025-05-05T05:45:00.519613Z","shell.execute_reply":"2025-05-05T05:50:43.908303Z"}},"outputs":[{"name":"stdout","text":"🔄 Encoding VI corpus...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 109/109 [02:46<00:00,  1.53s/it]\n","output_type":"stream"},{"name":"stdout","text":"🔄 Encoding EN corpus...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 112/112 [02:51<00:00,  1.53s/it]\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"\nvi_query_and_en_context_top_40 = vi_query_and_en_context(\n    train_df=train_df,\n    all_contexts=en_contexts,\n    corpus_embeddings=en_corpus_embeddings,\n    model=model,\n    tokenizer=tokenizer,\n    device=device,\n    top_k=40\n)\nvi_query_and_vi_context_top_40 = vi_query_and_vi_context(\n    train_df=train_df,\n    all_contexts=vi_contexts,\n    corpus_embeddings=vi_corpus_embeddings,\n    model=model,\n    tokenizer=tokenizer,\n    device=device,\n    top_k=40\n)\nen_query_and_vi_context_top_40 = en_query_and_vi_context(\n    train_df=train_df,\n    all_contexts=vi_contexts,\n    corpus_embeddings=vi_corpus_embeddings,\n    model=model,\n    tokenizer=tokenizer,\n    device=device,\n    top_k=40\n)\nen_query_and_en_context_top_40 = en_query_and_en_context(\n    train_df=train_df,\n    all_contexts=en_contexts,\n    corpus_embeddings=en_corpus_embeddings,\n    model=model,\n    tokenizer=tokenizer,\n    device=device,\n    top_k=40\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:50:43.910635Z","iopub.execute_input":"2025-05-05T05:50:43.910847Z","iopub.status.idle":"2025-05-05T05:56:08.998912Z","shell.execute_reply.started":"2025-05-05T05:50:43.910831Z","shell.execute_reply":"2025-05-05T05:56:08.998185Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 4000/4000 [01:22<00:00, 48.31it/s]\n100%|██████████| 4000/4000 [01:22<00:00, 48.31it/s]\n100%|██████████| 4000/4000 [01:19<00:00, 50.53it/s]\n100%|██████████| 4000/4000 [01:20<00:00, 49.83it/s]\n","output_type":"stream"}],"execution_count":84},{"cell_type":"code","source":"compute_recall_mrr_multi_gt(vi_query_and_en_context_top_40, k=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:58:45.722978Z","iopub.execute_input":"2025-05-05T05:58:45.723722Z","iopub.status.idle":"2025-05-05T05:58:45.916158Z","shell.execute_reply.started":"2025-05-05T05:58:45.723697Z","shell.execute_reply":"2025-05-05T05:58:45.915544Z"}},"outputs":[{"name":"stdout","text":"Accuracy@1:  0.2845\nAccuracy@5:  0.4677\nAccuracy@10: 0.5400\nRecall@10:   0.5400\nMRR@10:      0.3634\n","output_type":"stream"},{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"{'acc@1': 0.2845,\n 'acc@5': 0.46775,\n 'acc@10': 0.54,\n 'recall@10': 0.54,\n 'mrr@10': 0.36339613095238094}"},"metadata":{}}],"execution_count":85},{"cell_type":"code","source":"compute_recall_mrr_multi_gt(en_query_and_vi_context_top_40, k=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:58:47.741664Z","iopub.execute_input":"2025-05-05T05:58:47.742260Z","iopub.status.idle":"2025-05-05T05:58:47.974308Z","shell.execute_reply.started":"2025-05-05T05:58:47.742218Z","shell.execute_reply":"2025-05-05T05:58:47.973706Z"}},"outputs":[{"name":"stdout","text":"Accuracy@1:  0.3050\nAccuracy@5:  0.4770\nAccuracy@10: 0.5577\nRecall@10:   0.5577\nMRR@10:      0.3811\n","output_type":"stream"},{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"{'acc@1': 0.305,\n 'acc@5': 0.477,\n 'acc@10': 0.55775,\n 'recall@10': 0.55775,\n 'mrr@10': 0.3810831349206349}"},"metadata":{}}],"execution_count":86},{"cell_type":"code","source":"compute_recall_mrr_multi_gt(en_query_and_en_context_top_40, k=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:58:49.617037Z","iopub.execute_input":"2025-05-05T05:58:49.617289Z","iopub.status.idle":"2025-05-05T05:58:49.808717Z","shell.execute_reply.started":"2025-05-05T05:58:49.617272Z","shell.execute_reply":"2025-05-05T05:58:49.808169Z"}},"outputs":[{"name":"stdout","text":"Accuracy@1:  0.3610\nAccuracy@5:  0.5265\nAccuracy@10: 0.5925\nRecall@10:   0.5925\nMRR@10:      0.4324\n","output_type":"stream"},{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"{'acc@1': 0.361,\n 'acc@5': 0.5265,\n 'acc@10': 0.5925,\n 'recall@10': 0.5925,\n 'mrr@10': 0.43239563492063493}"},"metadata":{}}],"execution_count":87},{"cell_type":"code","source":"compute_recall_mrr_multi_gt(vi_query_and_vi_context_top_40, k=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:58:51.627978Z","iopub.execute_input":"2025-05-05T05:58:51.628560Z","iopub.status.idle":"2025-05-05T05:58:51.822037Z","shell.execute_reply.started":"2025-05-05T05:58:51.628533Z","shell.execute_reply":"2025-05-05T05:58:51.821307Z"}},"outputs":[{"name":"stdout","text":"Accuracy@1:  0.4820\nAccuracy@5:  0.6418\nAccuracy@10: 0.7060\nRecall@10:   0.7060\nMRR@10:      0.5513\n","output_type":"stream"},{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"{'acc@1': 0.482,\n 'acc@5': 0.64175,\n 'acc@10': 0.706,\n 'recall@10': 0.706,\n 'mrr@10': 0.55131875}"},"metadata":{}}],"execution_count":88},{"cell_type":"code","source":"import ast\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nmodel_name = \"BAAI/bge-m3\"\ncheckpoint_path = \"/kaggle/input/checkpoint3/checkpoint/loss1\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(checkpoint_path).to(device)\ntrain_df = pd.read_json(test_path, lines=True)\nvi_contexts, vi_corpus_embeddings = embedding_vi_corpus(train_df, model, tokenizer, device)\nen_contexts, en_corpus_embeddings = embedding_en_corpus(train_df, model, tokenizer, device)\nvi_query_and_en_context_top_40 = vi_query_and_en_context(\n    train_df=train_df,\n    all_contexts=en_contexts,\n    corpus_embeddings=en_corpus_embeddings,\n    model=model,\n    tokenizer=tokenizer,\n    device=device,\n    top_k=40\n)\nvi_query_and_vi_context_top_40 = vi_query_and_vi_context(\n    train_df=train_df,\n    all_contexts=vi_contexts,\n    corpus_embeddings=vi_corpus_embeddings,\n    model=model,\n    tokenizer=tokenizer,\n    device=device,\n    top_k=40\n)\nen_query_and_vi_context_top_40 = en_query_and_vi_context(\n    train_df=train_df,\n    all_contexts=vi_contexts,\n    corpus_embeddings=vi_corpus_embeddings,\n    model=model,\n    tokenizer=tokenizer,\n    device=device,\n    top_k=40\n)\nen_query_and_en_context_top_40 = en_query_and_en_context(\n    train_df=train_df,\n    all_contexts=en_contexts,\n    corpus_embeddings=en_corpus_embeddings,\n    model=model,\n    tokenizer=tokenizer,\n    device=device,\n    top_k=40\n)\n\nce_model_name = \"/kaggle/input/cross-lingual-cross-encoder/cross_encoder_epoch2\" # \"BAAI/bge-reranker-v2-m3\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nce_model = AutoModelForSequenceClassification.from_pretrained(ce_model_name).to(device)\nce_tokenizer = AutoTokenizer.from_pretrained(ce_model_name)\nce_model.eval()\n\ndef safe_list(s):\n    return ast.literal_eval(s) if isinstance(s, str) else s\n\ndef rerank(query, contexts, model, tokenizer, device, max_length=256):\n    pairs = [(query, ctx) for ctx in contexts]\n    inputs = tokenizer(\n        [q for q, c in pairs],\n        [c for q, c in pairs],\n        padding=True,\n        truncation=True,\n        max_length=max_length,\n        return_tensors=\"pt\"\n    ).to(device)\n\n    with torch.no_grad():\n        logits = model(**inputs).logits\n        scores = logits.squeeze()\n\n    ranked = sorted(zip(contexts, scores.cpu().tolist()), key=lambda x: x[1], reverse=True)\n    return [ctx for ctx, _ in ranked]\n\ndef rerank_top_k_results(df, model, tokenizer, device):\n    reranked_rows = []\n    for _, row in tqdm(df.iterrows(), total=len(df)):\n        query = row[\"query\"]\n        pos = safe_list(row[\"pos\"])\n        top_k_pred = safe_list(row[\"top_k_pred\"])\n\n        reranked_top_k = rerank(query, top_k_pred, model, tokenizer, device)\n\n        reranked_rows.append({\n            \"query\": query,\n            \"pos\": pos,\n            \"top_k_pred\": reranked_top_k\n        })\n\n    return pd.DataFrame(reranked_rows)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:05:06.718830Z","iopub.execute_input":"2025-05-05T06:05:06.719108Z","iopub.status.idle":"2025-05-05T06:07:26.215152Z","shell.execute_reply.started":"2025-05-05T06:05:06.719088Z","shell.execute_reply":"2025-05-05T06:07:26.214581Z"}},"outputs":[{"name":"stdout","text":"Encoding VI corpus...\nEncoding EN corpus...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 500/500 [00:10<00:00, 48.56it/s]\n100%|██████████| 500/500 [00:10<00:00, 48.49it/s]\n100%|██████████| 500/500 [00:09<00:00, 51.87it/s]\n100%|██████████| 500/500 [00:09<00:00, 51.31it/s]\n","output_type":"stream"}],"execution_count":93},{"cell_type":"code","source":"reranked_vi_en = rerank_top_k_results(vi_query_and_en_context_top_40, ce_model, ce_tokenizer, device)\ncompute_recall_mrr_multi_gt(reranked_vi_en, k=10)\nreranked_vi_vi = rerank_top_k_results(vi_query_and_vi_context_top_40, ce_model, ce_tokenizer, device)\ncompute_recall_mrr_multi_gt(reranked_vi_vi, k=10)\nreranked_en_vi = rerank_top_k_results(en_query_and_vi_context_top_40, ce_model, ce_tokenizer, device)\ncompute_recall_mrr_multi_gt(reranked_en_vi, k=10)\nreranked_en_en = rerank_top_k_results(en_query_and_en_context_top_40, ce_model, ce_tokenizer, device)\ncompute_recall_mrr_multi_gt(reranked_en_en, k=10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:07:26.216298Z","iopub.execute_input":"2025-05-05T06:07:26.217073Z","iopub.status.idle":"2025-05-05T06:39:41.061543Z","shell.execute_reply.started":"2025-05-05T06:07:26.217047Z","shell.execute_reply":"2025-05-05T06:39:41.060969Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 500/500 [08:03<00:00,  1.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy@1:  0.4400\nAccuracy@5:  0.6540\nAccuracy@10: 0.7220\nRecall@10:   0.7220\nMRR@10:      0.5271\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 500/500 [08:05<00:00,  1.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy@1:  0.5640\nAccuracy@5:  0.7420\nAccuracy@10: 0.8120\nRecall@10:   0.8120\nMRR@10:      0.6419\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 500/500 [08:04<00:00,  1.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy@1:  0.4340\nAccuracy@5:  0.6320\nAccuracy@10: 0.7080\nRecall@10:   0.7080\nMRR@10:      0.5185\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 500/500 [08:01<00:00,  1.04it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy@1:  0.4960\nAccuracy@5:  0.6520\nAccuracy@10: 0.7200\nRecall@10:   0.7200\nMRR@10:      0.5635\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":94,"output_type":"execute_result","data":{"text/plain":"{'acc@1': 0.496,\n 'acc@5': 0.652,\n 'acc@10': 0.72,\n 'recall@10': 0.72,\n 'mrr@10': 0.5635246031746033}"},"metadata":{}}],"execution_count":94},{"cell_type":"code","source":"import ast\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nmodel_name = \"BAAI/bge-m3\"\ncheckpoint_path = \"/kaggle/input/cross-lingual/checkpoint/loss1\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(checkpoint_path).to(device)\ntrain_df = pd.read_json(test_path, lines=True)\nvi_contexts, vi_corpus_embeddings = embedding_vi_corpus(train_df, model, tokenizer, device)\nen_contexts, en_corpus_embeddings = embedding_en_corpus(train_df, model, tokenizer, device)\nvi_query_and_en_context_top_40 = vi_query_and_en_context(\n    train_df=train_df,\n    all_contexts=en_contexts,\n    corpus_embeddings=en_corpus_embeddings,\n    model=model,\n    tokenizer=tokenizer,\n    device=device,\n    top_k=40\n)\nvi_query_and_vi_context_top_40 = vi_query_and_vi_context(\n    train_df=train_df,\n    all_contexts=vi_contexts,\n    corpus_embeddings=vi_corpus_embeddings,\n    model=model,\n    tokenizer=tokenizer,\n    device=device,\n    top_k=40\n)\nen_query_and_vi_context_top_40 = en_query_and_vi_context(\n    train_df=train_df,\n    all_contexts=vi_contexts,\n    corpus_embeddings=vi_corpus_embeddings,\n    model=model,\n    tokenizer=tokenizer,\n    device=device,\n    top_k=40\n)\nen_query_and_en_context_top_40 = en_query_and_en_context(\n    train_df=train_df,\n    all_contexts=en_contexts,\n    corpus_embeddings=en_corpus_embeddings,\n    model=model,\n    tokenizer=tokenizer,\n    device=device,\n    top_k=40\n)\n\nce_model_name = \"/kaggle/input/cross-lingual-cross-encoder/cross_encoder_epoch2\" # \"BAAI/bge-reranker-v2-m3\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nce_model = AutoModelForSequenceClassification.from_pretrained(ce_model_name).to(device)\nce_tokenizer = AutoTokenizer.from_pretrained(ce_model_name)\nce_model.eval()\n\ndef safe_list(s):\n    return ast.literal_eval(s) if isinstance(s, str) else s\n\ndef rerank(query, contexts, model, tokenizer, device, max_length=256):\n    pairs = [(query, ctx) for ctx in contexts]\n    inputs = tokenizer(\n        [q for q, c in pairs],\n        [c for q, c in pairs],\n        padding=True,\n        truncation=True,\n        max_length=max_length,\n        return_tensors=\"pt\"\n    ).to(device)\n\n    with torch.no_grad():\n        logits = model(**inputs).logits\n        scores = logits.squeeze()\n\n    ranked = sorted(zip(contexts, scores.cpu().tolist()), key=lambda x: x[1], reverse=True)\n    return [ctx for ctx, _ in ranked]\n\ndef rerank_top_k_results(df, model, tokenizer, device):\n    reranked_rows = []\n    for _, row in tqdm(df.iterrows(), total=len(df)):\n        query = row[\"query\"]\n        pos = safe_list(row[\"pos\"])\n        top_k_pred = safe_list(row[\"top_k_pred\"])\n\n        reranked_top_k = rerank(query, top_k_pred, model, tokenizer, device)\n\n        reranked_rows.append({\n            \"query\": query,\n            \"pos\": pos,\n            \"top_k_pred\": reranked_top_k\n        })\n\n    return pd.DataFrame(reranked_rows)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:23:07.254728Z","iopub.execute_input":"2025-05-05T07:23:07.255429Z","iopub.status.idle":"2025-05-05T07:25:25.660049Z","shell.execute_reply.started":"2025-05-05T07:23:07.255406Z","shell.execute_reply":"2025-05-05T07:25:25.659406Z"}},"outputs":[{"name":"stdout","text":"Encoding VI corpus...\nEncoding EN corpus...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 500/500 [00:10<00:00, 48.92it/s]\n100%|██████████| 500/500 [00:10<00:00, 49.06it/s]\n100%|██████████| 500/500 [00:09<00:00, 52.04it/s]\n100%|██████████| 500/500 [00:09<00:00, 51.71it/s]\n","output_type":"stream"}],"execution_count":99},{"cell_type":"code","source":"reranked_vi_en = rerank_top_k_results(vi_query_and_en_context_top_40, ce_model, ce_tokenizer, device)\ncompute_recall_mrr_multi_gt(reranked_vi_en, k=10)\nreranked_vi_vi = rerank_top_k_results(vi_query_and_vi_context_top_40, ce_model, ce_tokenizer, device)\ncompute_recall_mrr_multi_gt(reranked_vi_vi, k=10)\nreranked_en_vi = rerank_top_k_results(en_query_and_vi_context_top_40, ce_model, ce_tokenizer, device)\ncompute_recall_mrr_multi_gt(reranked_en_vi, k=10)\nreranked_en_en = rerank_top_k_results(en_query_and_en_context_top_40, ce_model, ce_tokenizer, device)\ncompute_recall_mrr_multi_gt(reranked_en_en, k=10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:25:25.661063Z","iopub.execute_input":"2025-05-05T07:25:25.661309Z","iopub.status.idle":"2025-05-05T07:57:36.554670Z","shell.execute_reply.started":"2025-05-05T07:25:25.661285Z","shell.execute_reply":"2025-05-05T07:57:36.554093Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 500/500 [08:02<00:00,  1.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy@1:  0.4600\nAccuracy@5:  0.6800\nAccuracy@10: 0.7580\nRecall@10:   0.7580\nMRR@10:      0.5478\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 500/500 [08:04<00:00,  1.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy@1:  0.5680\nAccuracy@5:  0.7440\nAccuracy@10: 0.8220\nRecall@10:   0.8220\nMRR@10:      0.6462\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 500/500 [08:03<00:00,  1.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy@1:  0.4260\nAccuracy@5:  0.6380\nAccuracy@10: 0.7160\nRecall@10:   0.7160\nMRR@10:      0.5151\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 500/500 [08:00<00:00,  1.04it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy@1:  0.4900\nAccuracy@5:  0.6560\nAccuracy@10: 0.7380\nRecall@10:   0.7380\nMRR@10:      0.5623\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":100,"output_type":"execute_result","data":{"text/plain":"{'acc@1': 0.49,\n 'acc@5': 0.656,\n 'acc@10': 0.738,\n 'recall@10': 0.738,\n 'mrr@10': 0.5622793650793652}"},"metadata":{}}],"execution_count":100},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}