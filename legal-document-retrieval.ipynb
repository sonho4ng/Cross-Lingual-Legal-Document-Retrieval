{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 11263344,
     "sourceType": "datasetVersion",
     "datasetId": 7040005
    }
   ],
   "dockerImageVersionId": 30918,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "## PREPROCESS\n# chia dataset ra\n# th√™m negative l√† d·ª± ƒëo√°n c·ªßa bge-m3\n\n## HYBRID SEARCH\n# d√πng BM25 ƒë·ªÉ l·∫•y top 50\n# t·∫°o Dataset class ph√π h·ª£p v√≥i triplet loss\n# fine tune bi-encoder PhoBERT triplet loss v·ªõi LoRA v√† l·∫•y top 30\n# h·ª£p nh·∫•t v√† l·∫•y top 40\n\n## RERANK\n# t·∫°o Dataset class ph√π h·ª£p v√≥i binary classification\n# fine tune cross encoder PhoBERT v·ªõi binary classification\n\n# EVAL\n%env CUDA_LAUNCH_BLOCKING=1\n",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-09T05:17:22.200222Z",
     "iopub.execute_input": "2025-04-09T05:17:22.200527Z",
     "iopub.status.idle": "2025-04-09T05:17:22.205693Z",
     "shell.execute_reply.started": "2025-04-09T05:17:22.200495Z",
     "shell.execute_reply": "2025-04-09T05:17:22.204738Z"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# PREPROCESS",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\ncorpus = pd.read_csv(\"/kaggle/input/retrieval/corpus.csv\")\ntrain = pd.read_csv(\"/kaggle/input/retrieval/train.csv\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-09T05:17:22.206635Z",
     "iopub.execute_input": "2025-04-09T05:17:22.206884Z",
     "iopub.status.idle": "2025-04-09T05:17:40.770256Z",
     "shell.execute_reply.started": "2025-04-09T05:17:22.206860Z",
     "shell.execute_reply": "2025-04-09T05:17:40.769306Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## DATASET",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def prepare_data(train_df, corpus_df, sample_size=500, max_corpus_size=10000, output_prefix=\"hybrid_search\", random_state=42):\n",
    "\n",
    "    train_sampled = train_df.sample(n=sample_size, random_state=random_state)\n",
    "\n",
    "\n",
    "    def robust_clean_cid_string(s):\n",
    "        if not isinstance(s, str):\n",
    "            return []\n",
    "        s = s.replace('[', '').replace(']', '').replace(\"'\", '').replace('\"', '').strip()\n",
    "        s = re.sub(r'\\s+', ',', s)\n",
    "        try:\n",
    "            return [int(x) for x in s.split(',') if x.strip().isdigit()]\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è L·ªói parse cid: {s} ‚Üí {e}\")\n",
    "            return []\n",
    "\n",
    "    train_sampled['cid'] = train_sampled['cid'].apply(robust_clean_cid_string)\n",
    "\n",
    "\n",
    "    available_cids = set(corpus_df['cid'].unique())\n",
    "    train_sampled = train_sampled[train_sampled['cid'].apply(lambda cids: all(cid in available_cids for cid in cids))]\n",
    "\n",
    "\n",
    "    all_cids = set(cid for sublist in train_sampled['cid'] for cid in sublist)\n",
    "    cid_to_text = corpus_df.set_index(\"cid\")['text'].to_dict()\n",
    "    train_sampled['positive_context'] = train_sampled['cid'].apply(\n",
    "        lambda cids: \"\\n\".join([cid_to_text.get(cid, \"\") for cid in cids if cid in cid_to_text])\n",
    "    )\n",
    "\n",
    "\n",
    "    corpus_filtered = corpus_df[corpus_df['cid'].isin(all_cids)].copy()\n",
    "    corpus_sampled = corpus_filtered.sample(n=1000, random_state=42) if len(corpus_filtered) > max_corpus_size else corpus_filtered\n",
    "\n",
    "\n",
    "    train_sampled = train_sampled.dropna(subset=['question', 'positive_context'])\n",
    "    train_sampled.to_csv(f\"train_{output_prefix}.csv\", index=False)\n",
    "    corpus_sampled.to_csv(f\"corpus_{output_prefix}.csv\", index=False)\n",
    "\n",
    "    print(f\"‚úÖ Saved: {len(train_sampled)} training rows and {len(corpus_sampled)} corpus rows.\")\n",
    "\n",
    "\n",
    "prepare_data(train, corpus, sample_size=500, output_prefix=\"hybrid_search\", random_state=123)\n",
    "prepare_data(train, corpus, sample_size=500, output_prefix=\"rerank\", random_state=456)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-09T05:17:40.771496Z",
     "iopub.execute_input": "2025-04-09T05:17:40.771760Z",
     "iopub.status.idle": "2025-04-09T05:17:41.578036Z",
     "shell.execute_reply.started": "2025-04-09T05:17:40.771740Z",
     "shell.execute_reply": "2025-04-09T05:17:41.577305Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "def check_cid_consistency(train_path, corpus_path):\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    corpus_df = pd.read_csv(corpus_path)\n",
    "\n",
    "    # Chu·∫©n h√≥a c·ªôt cid t·ª´ string ‚Üí list[int]\n",
    "    def parse_cid(x):\n",
    "        if isinstance(x, list):\n",
    "            return x\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    train_df['cid'] = train_df['cid'].apply(parse_cid)\n",
    "\n",
    "    # T·∫≠p h·ª£p t·∫•t c·∫£ cid xu·∫•t hi·ªán trong train\n",
    "    train_cids = set(cid for sublist in train_df['cid'] for cid in sublist)\n",
    "\n",
    "    # T·∫≠p h·ª£p to√†n b·ªô cid trong corpus\n",
    "    corpus_cids = set(corpus_df['cid'].unique())\n",
    "\n",
    "    # So s√°nh\n",
    "    missing_cids = train_cids - corpus_cids\n",
    "\n",
    "    if not missing_cids:\n",
    "        print(\"‚úÖ T·∫•t c·∫£ CID trong train ƒë·ªÅu c√≥ trong corpus.\")\n",
    "    else:\n",
    "        print(f\"C√≥ {len(missing_cids)} CID b·ªã thi·∫øu trong corpus:\")\n",
    "    return missing_cids\n",
    "check_cid_consistency(train_path=\"train_rerank.csv\", corpus_path=\"corpus_rerank.csv\")\n",
    "check_cid_consistency(train_path=\"train_hybrid_search.csv\", corpus_path=\"corpus_hybrid_search.csv\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-09T05:17:41.579000Z",
     "iopub.execute_input": "2025-04-09T05:17:41.579349Z",
     "iopub.status.idle": "2025-04-09T05:17:41.699759Z",
     "shell.execute_reply.started": "2025-04-09T05:17:41.579291Z",
     "shell.execute_reply": "2025-04-09T05:17:41.699054Z"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## th√™m hard negative b·∫±ng d·ª± ƒëo√°n c·ªßa bge-m3",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from transformers import AutoTokenizer, AutoModel\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom tqdm import tqdm\n\ndef generate_hard_negatives_with_bge(\n    train_file: str,\n    corpus_file: str,\n    output_file: str = \"train_neg_hybrid_search.csv\",\n    model_name: str = \"BAAI/bge-m3\",\n    batch_size: int = 64,\n    max_length: int = 256\n):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Load model & tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModel.from_pretrained(model_name).to(device)\n    model.eval()\n\n    # Load data\n    train_sampled = pd.read_csv(train_file)\n    corpus_sampled = pd.read_csv(corpus_file)\n\n    # L·∫•y text corpus v√† map cid\n    cid_to_text = corpus_sampled.set_index(\"cid\")['text'].to_dict()\n    all_corpus_texts = list(cid_to_text.values())\n    all_corpus_cids = list(cid_to_text.keys())\n\n    # H√†m encode\n    def get_embedding(texts):\n        inputs = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt').to(device)\n        with torch.no_grad():\n            outputs = model(**inputs)\n            return outputs.last_hidden_state[:, 0].cpu().numpy()  # [CLS]\n\n    # Embed corpus\n    print(\"üîÑ Encoding corpus...\")\n    corpus_embeddings = []\n    for i in tqdm(range(0, len(all_corpus_texts), batch_size)):\n        batch_texts = all_corpus_texts[i:i+batch_size]\n        batch_emb = get_embedding(batch_texts)\n        corpus_embeddings.append(batch_emb)\n    corpus_embeddings = np.vstack(corpus_embeddings)\n\n    # T·∫°o hard negatives\n    print(\"üîç Generating hard negatives...\")\n    negatives = []\n\n    for idx, row in tqdm(train_sampled.iterrows(), total=len(train_sampled)):\n        question = row['question']\n        pos_cids = set(row['cid']) if isinstance(row['cid'], list) else set()\n\n        q_emb = get_embedding([question])\n        sims = cosine_similarity(q_emb, corpus_embeddings)[0]\n        top_indices = sims.argsort()[::-1]\n\n        selected_neg = None\n        for top_idx in top_indices:\n            neg_cid = all_corpus_cids[top_idx]\n            if neg_cid not in pos_cids:\n                selected_neg = all_corpus_texts[top_idx]\n                break\n\n        negatives.append(selected_neg or \"no hard negative found\")\n\n    # G√°n v√† l∆∞u\n    train_sampled['negative'] = negatives\n    train_sampled = train_sampled.dropna(subset=['question', 'positive_context', 'negative'])\n    train_sampled = train_sampled.drop(columns=['positive_context'])\n    train_sampled.to_csv(output_file, index=False)\n\n    print(f\"‚úÖ ƒê√£ t·∫°o negative v√† l∆∞u: {output_file} ({len(train_sampled)} samples)\")\n    return train_sampled\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-09T05:17:41.701933Z",
     "iopub.execute_input": "2025-04-09T05:17:41.702147Z",
     "iopub.status.idle": "2025-04-09T05:17:49.694839Z",
     "shell.execute_reply.started": "2025-04-09T05:17:41.702129Z",
     "shell.execute_reply": "2025-04-09T05:17:49.694163Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "generate_hard_negatives_with_bge(\n    train_file=\"train_hybrid_search.csv\",\n    corpus_file=\"corpus_hybrid_search.csv\",\n    output_file=\"train_neg_hybrid_search.csv\"\n)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-09T05:17:49.696015Z",
     "iopub.execute_input": "2025-04-09T05:17:49.696454Z",
     "iopub.status.idle": "2025-04-09T05:18:43.816004Z",
     "shell.execute_reply.started": "2025-04-09T05:17:49.696409Z",
     "shell.execute_reply": "2025-04-09T05:18:43.815026Z"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "generate_hard_negatives_with_bge(\n    train_file=\"train_rerank.csv\",\n    corpus_file=\"corpus_rerank.csv\",\n    output_file=\"train_neg_rerank.csv\"\n)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-09T05:18:43.816985Z",
     "iopub.execute_input": "2025-04-09T05:18:43.817274Z",
     "iopub.status.idle": "2025-04-09T05:19:15.368342Z",
     "shell.execute_reply.started": "2025-04-09T05:18:43.817250Z",
     "shell.execute_reply": "2025-04-09T05:19:15.367460Z"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "\n# Load file\ndf = pd.read_csv(\"train_neg_rerank.csv\")  # ho·∫∑c t√™n file b·∫°n ƒëang d√πng\n\n# T·∫°o 2 b·∫£n ghi cho m·ªói d√≤ng: 1 positive v√† 1 negative\npos_rows = df[[\"question\", \"context\"]].copy()\npos_rows[\"label\"] = 1\npos_rows = pos_rows.rename(columns={\"context\": \"context\"})\n\nneg_rows = df[[\"question\", \"negative\"]].copy()\nneg_rows[\"label\"] = 0\nneg_rows = neg_rows.rename(columns={\"negative\": \"context\"})\n\n# G·ªôp l·∫°i\nrerank_df = pd.concat([pos_rows, neg_rows], ignore_index=True)\n\n# Shuffle ƒë·ªÉ tr√°nh model bias\nrerank_df = rerank_df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n\n# L∆∞u\nrerank_df.to_csv(\"rerank_train.csv\", index=False)\nprint(\"‚úÖ Saved rerank_train.csv with shape:\", rerank_df.shape)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-09T05:19:15.369464Z",
     "iopub.execute_input": "2025-04-09T05:19:15.369843Z",
     "iopub.status.idle": "2025-04-09T05:19:15.457839Z",
     "shell.execute_reply.started": "2025-04-09T05:19:15.369807Z",
     "shell.execute_reply": "2025-04-09T05:19:15.456918Z"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# HYBRID SEARCH",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## BM25 ƒë·ªÉ l·∫•y top 20",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install rank_bm25",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-09T05:19:15.458786Z",
     "iopub.execute_input": "2025-04-09T05:19:15.459108Z",
     "iopub.status.idle": "2025-04-09T05:19:19.757023Z",
     "shell.execute_reply.started": "2025-04-09T05:19:15.459080Z",
     "shell.execute_reply": "2025-04-09T05:19:19.756144Z"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from rank_bm25 import BM25Okapi\ndef retrieve_bm25(query, corpus, top_k=20):\n    corpus = pd.read_csv(corpus)\n    bm25 = BM25Okapi(corpus[\"text\"].str.split())\n    scores = bm25.get_scores(query.split())\n    top_idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n    return corpus.iloc[top_idx]",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-09T05:19:19.758017Z",
     "iopub.execute_input": "2025-04-09T05:19:19.758254Z",
     "iopub.status.idle": "2025-04-09T05:19:19.767441Z",
     "shell.execute_reply.started": "2025-04-09T05:19:19.758233Z",
     "shell.execute_reply": "2025-04-09T05:19:19.766739Z"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Bi-encoder ƒë·ªÉ l·∫•y top 10",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from torch.utils.data import Dataset\nimport pandas as pd\n\nclass LegalBiEncoderDataset(Dataset):\n    def __init__(self, file_path, tokenizer, max_length):\n        self.data = pd.read_csv(file_path)\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        question = row['question']\n        pos = row['context']\n        neg = row['negative']\n\n        anchor = self.tokenizer(question, padding='max_length', truncation=True,\n                                max_length=self.max_length, return_tensors=\"pt\")\n        positive = self.tokenizer(pos, padding='max_length', truncation=True,\n                                  max_length=self.max_length, return_tensors=\"pt\")\n        negative = self.tokenizer(neg, padding='max_length', truncation=True,\n                                  max_length=self.max_length, return_tensors=\"pt\")\n\n        return {\n            \"anchor_input_ids\": anchor[\"input_ids\"].squeeze(),\n            \"anchor_attention_mask\": anchor[\"attention_mask\"].squeeze(),\n            \"positive_input_ids\": positive[\"input_ids\"].squeeze(),\n            \"positive_attention_mask\": positive[\"attention_mask\"].squeeze(),\n            \"negative_input_ids\": negative[\"input_ids\"].squeeze(),\n            \"negative_attention_mask\": negative[\"attention_mask\"].squeeze()\n        }\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-09T05:19:19.768153Z",
     "iopub.execute_input": "2025-04-09T05:19:19.768394Z",
     "iopub.status.idle": "2025-04-09T05:19:19.781343Z",
     "shell.execute_reply.started": "2025-04-09T05:19:19.768374Z",
     "shell.execute_reply": "2025-04-09T05:19:19.780588Z"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport torch.nn as nn\nfrom transformers import AutoModel\nfrom peft import get_peft_model, LoraConfig, TaskType\n\nclass BiEncoderPhoBERT(nn.Module):\n    def __init__(self, model_name):\n        super().__init__()\n        base_model = AutoModel.from_pretrained(model_name)\n\n        # LoRA config\n        peft_config = LoraConfig(\n            task_type=TaskType.FEATURE_EXTRACTION,\n            r=8,\n            lora_alpha=16,\n            lora_dropout=0.1,\n            target_modules=[\"encoder.layer.11.attention.output.dense\"],  # ho·∫∑c m·ªü r·ªông th√™m\n            bias=\"none\"\n        )\n        self.encoder = get_peft_model(base_model, peft_config)\n\n    def forward(self, input_ids, attention_mask):\n        output = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = output.last_hidden_state[:, 0]  # [CLS] vector\n        return cls_output\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-09T05:19:19.782207Z",
     "iopub.execute_input": "2025-04-09T05:19:19.782450Z",
     "iopub.status.idle": "2025-04-09T05:19:20.185572Z",
     "shell.execute_reply.started": "2025-04-09T05:19:19.782419Z",
     "shell.execute_reply": "2025-04-09T05:19:20.184894Z"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer\nfrom tqdm import tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_name = \"vinai/phobert-base\"\nmax_length = 256\nepochs = 10\nbatch_size = 32\nlr = 2e-5\n\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\ndataset = LegalBiEncoderDataset(\"/kaggle/working/train_neg_hybrid_search.csv\", tokenizer, max_length)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\nmodel = BiEncoderPhoBERT(model_name).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr)\nloss_fn = nn.TripletMarginLoss(margin=0.3)\n\n# Training loop\nmodel.train()\nfor epoch in range(epochs):\n    total_loss = 0\n    for batch in tqdm(dataloader):\n        anchor = model(batch['anchor_input_ids'].to(device), batch['anchor_attention_mask'].to(device))\n        pos = model(batch['positive_input_ids'].to(device), batch['positive_attention_mask'].to(device))\n        neg = model(batch['negative_input_ids'].to(device), batch['negative_attention_mask'].to(device))\n\n        loss = loss_fn(anchor, pos, neg)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        total_loss += loss.item()\n    \n    print(f\"Epoch {epoch + 1}: Loss = {total_loss:.4f}\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-09T05:19:20.186376Z",
     "iopub.execute_input": "2025-04-09T05:19:20.186595Z",
     "iopub.status.idle": "2025-04-09T05:21:46.783681Z",
     "shell.execute_reply.started": "2025-04-09T05:19:20.186576Z",
     "shell.execute_reply": "2025-04-09T05:21:46.782734Z"
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "\nimport os\noutput_dir = \"checkpoints/bi_encoder_phobert\"\nos.makedirs(output_dir, exist_ok=True)\n\nmodel.encoder.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\nprint(f\"‚úÖ ƒê√£ l∆∞u checkpoint t·∫°i {output_dir}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-09T05:21:46.786638Z",
     "iopub.execute_input": "2025-04-09T05:21:46.786885Z",
     "iopub.status.idle": "2025-04-09T05:21:47.239962Z",
     "shell.execute_reply.started": "2025-04-09T05:21:46.786862Z",
     "shell.execute_reply": "2025-04-09T05:21:47.239216Z"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from peft import PeftModel, PeftConfig\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "corpus_df = pd.read_csv(\"/kaggle/working/corpus_rerank.csv\")\n",
    "corpus_texts = corpus_df[\"text\"].tolist()\n",
    "corpus_cids = corpus_df[\"cid\"].tolist()\n",
    "\n",
    "def load_dense_model(checkpoint_path=\"checkpoints/bi_encoder_phobert\"):\n",
    "    config = PeftConfig.from_pretrained(checkpoint_path)\n",
    "    base = AutoModel.from_pretrained(config.base_model_name_or_path)\n",
    "    model = PeftModel.from_pretrained(base, checkpoint_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint_path, use_fast=False)\n",
    "    return model.eval(), tokenizer\n",
    "\n",
    "def encode_corpus(model, tokenizer, device, max_len=256):\n",
    "    embeddings = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text in corpus_texts:\n",
    "            inputs = tokenizer(text, truncation=True, padding='max_length', max_length=max_len, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            output = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "            cls_output = output.last_hidden_state[:, 0]  # [CLS] token\n",
    "            emb = cls_output.cpu().numpy()[0]\n",
    "            embeddings.append(emb)\n",
    "\n",
    "    return torch.tensor(embeddings)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, tokenizer = load_dense_model()\n",
    "corpus_embeddings = encode_corpus(model, tokenizer, device)\n",
    "\n",
    "\n",
    "def dense_func(query, top_k=10):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(query, truncation=True, padding='max_length', max_length=256, return_tensors=\"pt\").to(device)\n",
    "        output = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "        cls_output = output.last_hidden_state[:, 0]  # l·∫•y vector [CLS]\n",
    "        query_vec = cls_output.cpu().numpy()\n",
    "\n",
    "    sims = cosine_similarity(query_vec, corpus_embeddings.numpy())[0]\n",
    "    top_indices = sims.argsort()[::-1][:top_k]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            \"cid\": corpus_cids[idx],\n",
    "            \"text\": corpus_texts[idx],\n",
    "            \"score\": float(sims[idx]) # ch·ªâ ƒë·ªÉ debug th√¥i, sau x√≥a\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-09T05:21:47.241130Z",
     "iopub.execute_input": "2025-04-09T05:21:47.241397Z",
     "iopub.status.idle": "2025-04-09T05:22:01.462090Z",
     "shell.execute_reply.started": "2025-04-09T05:21:47.241375Z",
     "shell.execute_reply": "2025-04-09T05:22:01.460851Z"
    }
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def hybrid_search(query, bm25_func, dense_func, corpus):\n",
    "    bm25_res = bm25_func(query, corpus)          \n",
    "    dense_res = dense_func(query)        \n",
    "\n",
    "    bm25_list = bm25_res.to_dict(orient=\"records\")\n",
    "\n",
    "    merged = {item[\"cid\"]: item for item in bm25_list + dense_res}\n",
    "\n",
    "    # ƒëang ∆∞u ti√™n bm25\n",
    "    return list(merged.values())[:25]\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-09T05:22:01.463345Z",
     "iopub.execute_input": "2025-04-09T05:22:01.463697Z",
     "iopub.status.idle": "2025-04-09T05:22:01.468225Z",
     "shell.execute_reply.started": "2025-04-09T05:22:01.463662Z",
     "shell.execute_reply": "2025-04-09T05:22:01.467405Z"
    }
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### TESTING",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# testing\nquery = \"Ph√≥ T·ªïng Gi√°m ƒë·ªëc Ng√¢n h√†ng Ch√≠nh s√°ch x√£ h·ªôi ƒë∆∞·ª£c x·∫øp l∆∞∆°ng theo b·∫£ng l∆∞∆°ng nh∆∞ th·∫ø n√†o?\"\nresults = dense_func(query)\nresults",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-09T05:22:01.469117Z",
     "iopub.execute_input": "2025-04-09T05:22:01.469394Z",
     "iopub.status.idle": "2025-04-09T05:22:01.517131Z",
     "shell.execute_reply.started": "2025-04-09T05:22:01.469368Z",
     "shell.execute_reply": "2025-04-09T05:22:01.516266Z"
    }
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "## testing\nquery = \"Ph√≥ T·ªïng Gi√°m ƒë·ªëc Ng√¢n h√†ng Ch√≠nh s√°ch x√£ h·ªôi ƒë∆∞·ª£c x·∫øp l∆∞∆°ng theo b·∫£ng l∆∞∆°ng nh∆∞ th·∫ø n√†o?\"\ncorpus = \"/kaggle/working/corpus_hybrid_search.csv\"\nbm25_results = retrieve_bm25(query,corpus)\nbm25_results",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-09T05:22:01.517950Z",
     "iopub.execute_input": "2025-04-09T05:22:01.518185Z",
     "iopub.status.idle": "2025-04-09T05:22:01.894820Z",
     "shell.execute_reply.started": "2025-04-09T05:22:01.518164Z",
     "shell.execute_reply": "2025-04-09T05:22:01.894084Z"
    }
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "## testing\nquery = \"Ph√≥ T·ªïng Gi√°m ƒë·ªëc Ng√¢n h√†ng Ch√≠nh s√°ch x√£ h·ªôi ƒë∆∞·ª£c x·∫øp l∆∞∆°ng theo b·∫£ng l∆∞∆°ng nh∆∞ th·∫ø n√†o?\"\nhybrid_searching = hybrid_search(query, retrieve_bm25, dense_func, corpus )\nhybrid_searching",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-09T05:22:31.914617Z",
     "iopub.execute_input": "2025-04-09T05:22:31.914929Z",
     "iopub.status.idle": "2025-04-09T05:22:32.040248Z",
     "shell.execute_reply.started": "2025-04-09T05:22:31.914907Z",
     "shell.execute_reply": "2025-04-09T05:22:32.039384Z"
    }
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# RERANK\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from torch.utils.data import Dataset\nimport pandas as pd\n\nclass RerankDataset(Dataset):\n    def __init__(self, file_path, tokenizer, max_length=256):\n        self.data = pd.read_csv(file_path)\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        question = row[\"question\"]\n        context = row[\"context\"]\n        label = row[\"label\"]  # 1 = relevant, 0 = not relevant\n\n        encoded = self.tokenizer(\n            question,\n            context,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n\n        return {\n            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n            \"label\": int(label)\n        }\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-09T05:22:36.997355Z",
     "iopub.execute_input": "2025-04-09T05:22:36.997675Z",
     "iopub.status.idle": "2025-04-09T05:22:37.003672Z",
     "shell.execute_reply.started": "2025-04-09T05:22:36.997649Z",
     "shell.execute_reply": "2025-04-09T05:22:37.002579Z"
    }
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\"Be aware, overflowing tokens.*\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-09T05:22:37.412135Z",
     "iopub.execute_input": "2025-04-09T05:22:37.412501Z",
     "iopub.status.idle": "2025-04-09T05:22:37.417015Z",
     "shell.execute_reply.started": "2025-04-09T05:22:37.412469Z",
     "shell.execute_reply": "2025-04-09T05:22:37.416168Z"
    }
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom transformers import AutoTokenizer\nfrom sklearn.model_selection import train_test_split\n\nmodel_name = \"vinai/phobert-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n\n# Load dataset\ndf = pd.read_csv(\"rerank_train.csv\")  # C·ªôt: question, context, label\ntrain_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n\ntrain_df.to_csv(\"rerank_train_split.csv\", index=False)\nval_df.to_csv(\"rerank_val_split.csv\", index=False)\n\ntrain_dataset = RerankDataset(\"rerank_train_split.csv\", tokenizer)\nval_dataset = RerankDataset(\"rerank_val_split.csv\", tokenizer)\n\n# Load model\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\n# Training args\ntraining_args = TrainingArguments(\n    output_dir=\"./rerank_ckpt\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    save_total_limit=2\n)\n\n# Metric\nfrom sklearn.metrics import accuracy_score, f1_score\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds)\n    }\n\n# Trainer\nfrom transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n\n# Train\ntrainer.train()\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-09T05:22:38.488254Z",
     "iopub.execute_input": "2025-04-09T05:22:38.488611Z",
     "iopub.status.idle": "2025-04-09T05:24:34.374757Z",
     "shell.execute_reply.started": "2025-04-09T05:22:38.488587Z",
     "shell.execute_reply": "2025-04-09T05:24:34.374027Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "\n",
    "def rerank_with_cross_encoder(question, contexts, model_path, model_name=\"vinai/phobert-base\", top_k=5, device=None, max_length=256):\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device).eval()\n",
    "\n",
    "\n",
    "    contexts = [c if isinstance(c, str) else \"\" for c in contexts]\n",
    "    contexts = [c[:2048] for c in contexts]  # ngƒÉn tokenizer t·∫°o >512 tokens\n",
    "\n",
    "    try:\n",
    "        inputs = tokenizer(\n",
    "            [question] * len(contexts),\n",
    "            contexts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "    except Exception as e:\n",
    "        print(\"Tokenizer error:\", e)\n",
    "        print(\"Context length:\", [len(c) for c in contexts])\n",
    "        raise\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)[:, 1]\n",
    "\n",
    "    results = list(zip(contexts, probs.cpu().tolist()))\n",
    "    reranked = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return reranked[:top_k]\n",
    "\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-09T05:24:34.375959Z",
     "iopub.execute_input": "2025-04-09T05:24:34.376198Z",
     "iopub.status.idle": "2025-04-09T05:24:34.383268Z",
     "shell.execute_reply.started": "2025-04-09T05:24:34.376176Z",
     "shell.execute_reply": "2025-04-09T05:24:34.382601Z"
    }
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "best_ckpt_path = trainer.state.best_model_checkpoint\nprint(\"‚úÖ Best checkpoint path:\", best_ckpt_path)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-09T05:24:34.384495Z",
     "iopub.execute_input": "2025-04-09T05:24:34.384766Z",
     "iopub.status.idle": "2025-04-09T05:24:34.402884Z",
     "shell.execute_reply.started": "2025-04-09T05:24:34.384735Z",
     "shell.execute_reply": "2025-04-09T05:24:34.402148Z"
    }
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# M·ªôt example\nquestion = \"Ph√≥ T·ªïng Gi√°m ƒë·ªëc Ng√¢n h√†ng Ch√≠nh s√°ch x√£ h·ªôi ƒë∆∞·ª£c x·∫øp l∆∞∆°ng theo b·∫£ng l∆∞∆°ng nh∆∞ th·∫ø n√†o?\"\ncontexts = [item['text'] for item in hybrid_searching]\n\nreranked = rerank_with_cross_encoder(\n    question=question,\n    contexts=contexts,\n    model_path=best_ckpt_path,  # t√¥i kh√¥ng bi·∫øt t√™n checkpoint\n    model_name=model_name,\n    top_k=3,\n    device=\"cpu\"\n)\n\n# In k·∫øt qu·∫£\nfor i, (ctx, score) in enumerate(reranked, 1):\n    print(f\"#{i} - score: {score:.4f}\")\n    print(ctx)\n    print(\"-\" * 50)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-09T05:24:34.403767Z",
     "iopub.execute_input": "2025-04-09T05:24:34.403971Z",
     "iopub.status.idle": "2025-04-09T05:24:41.883232Z",
     "shell.execute_reply.started": "2025-04-09T05:24:34.403953Z",
     "shell.execute_reply": "2025-04-09T05:24:41.882277Z"
    }
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
